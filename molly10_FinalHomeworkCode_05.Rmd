---
title: "molly10_FinalHomeworkCode_05"
author: "Yinuo Mao"
date: "2025-04-10"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(boot)
```

### Using the â€œKamilarAndCooperData.csvâ€ dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your ğ›½ coeffiecients (slope and intercept).
```{r}
# Read in data
kamilar_cooper <- read.csv("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/refs/heads/master/AN588_Spring25/KamilarAndCooperData.csv")

# Create filtered_data by removing NA values
filtered_data <- kamilar_cooper[!is.na(kamilar_cooper$HomeRange_km2) & 
                               !is.na(kamilar_cooper$Body_mass_female_mean),]

# Run linear regression
lm_model<-lm(log(HomeRange_km2)~log(Body_mass_female_mean), data=filtered_data)

# Report the coefficients
coef(lm_model)
```

### Use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each ğ›½ coefficient.Estimate the standard error for each of your ğ›½ coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your ğ›½ coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
# Set seed for reproducibility
set.seed(123)

# Number of bootstrap replicates
n_boot <- 1000

# Initialize vectors to store the bootstrap estimates
boot_intercepts <- numeric(n_boot)
boot_slopes <- numeric(n_boot)

# Run bootstrap
for (i in 1:n_boot) {
  # Sample with replacement
  boot_indices <- sample(nrow(filtered_data), replace = TRUE)
  boot_sample <- filtered_data[boot_indices, ]
  
  # Fit the model on bootstrap sample - use the same formula as your original model
  boot_model <- lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = boot_sample)
  
  # Store the coefficients
  boot_intercepts[i] <- coef(boot_model)[1]
  boot_slopes[i] <- coef(boot_model)[2]
}
```

### How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?
For the intercept coefficient, the bootstrap SE is smaller than the SE from the linear model. This suggests that the linear model might be slightly overestimating the uncertainty in the intercept. For the slope coefficient, the bootstrap SE is a little closer to the the SE from the linear model compared with the intercept coefficient but still about 5-10% smaller. The smaller bootstrap SEs suggest that the data may actually have less variability than what would be expected under the strict parametric assumptions, particularly for the intercept estimate.

### How does the latter compare to the 95% CI estimated from your entire dataset?
For the intercept coefficient, the bootstrap CI is slightly narrower and less symmetric than the parametric CI, with a higher lower bound, means there is more certainty about the minimum intercept value. Similarly, for the slope coefficient, the bootstrap CI has a higher lower bound while remains a similar upper bound compared to the parametric CI, indicating greater confidence in the minimum strength of the relationship between body mass and home range size. These patterns suggest that linear model, which rely on assumptions of normality and homoscedasticity, may be slightly overestimating the uncertainty in our regression parameters. However, the bootstrap method, by directly sampling from the observed data without making these assumptions, provide a more data-driven representation of uncertainty that show a stronger and more definite relationship between primate body mass and home range size than indicated by standard linear model CIs.

## Challenges
1. At first, I found it really hard to grasp what bootstrapping actually does and why we need to sample with replacement. It took me a while to understand that we're trying to simulate having multiple datasets.
2. Trying to write the bootstrap code from scratch with for-loops was super frustrating! I kept getting errors about data types and subsetting. The loop would either crash or run forever without giving me results.
3. Once I got all the numbers, figuring out what the differences between bootstrap SE and regular SE actually meant for my analysis wasn't straightforward. I wasn't sure if smaller bootstrap SEs were good or bad.
4. Creating the function to extract coefficients for the boot() function was tricky. I had to make sure it returned exactly what I needed in the right format.
5. Getting the confidence intervals to display nicely in my final comparison table was surprisingly difficult. I had to figure out how to extract the values from boot.ci() output and convert them to a readable format.


# EXTRA CREDIT

### Write a FUNCTION that takes as its arguments a dataframe, â€œdâ€, a linear model, â€œmâ€ (as a character string, e.g., â€œlogHR~logBMâ€), a user-defined confidence interval level, â€œconf.levelâ€ (with default = 0.95), and a number of bootstrap replicates, â€œnâ€ (with default = 1000). Your function should return a dataframe that includes: beta coefficient names; beta coefficients, standard errors, and upper and lower CI limits for the linear model based on your entire dataset; and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.
```{r}
# Create log-transformed variables
filtered_data$logHomeRange <- log(filtered_data$HomeRange_km2)
filtered_data$logBodyMass <- log(filtered_data$Body_mass_female_mean)

# Function for bootstrap analysis of linear models
bootstrap_lm <- function(d, m, conf.level = 0.95, n = 1000) {
  # Fit model on full dataset
  full_model <- lm(as.formula(m), data = d)
  coef_names <- names(coef(full_model))
  
  # Get coefficients, SEs, and CIs from original model
  original_coefs <- coef(full_model)
  original_se <- sqrt(diag(vcov(full_model)))
  original_ci <- confint(full_model, level = conf.level)
  
  # Initialize matrix for bootstrap coefficients
  boot_coefs <- matrix(NA, nrow = n, ncol = length(coef_names))
  colnames(boot_coefs) <- coef_names
  
  # Perform bootstrap
  for (i in 1:n) {
    # Sample with replacement
    boot_indices <- sample(nrow(d), replace = TRUE)
    boot_sample <- d[boot_indices, ]
    
    # Fit model and put in coefficients
    boot_coefs[i, ] <- coef(lm(as.formula(m), data = boot_sample))
  }
  
  # Calculate bootstrap statistics
  boot_means <- colMeans(boot_coefs)
  boot_se <- apply(boot_coefs, 2, sd)
  alpha <- 1 - conf.level
  boot_ci_lower <- apply(boot_coefs, 2, quantile, probs = alpha/2)
  boot_ci_upper <- apply(boot_coefs, 2, quantile, probs = 1 - alpha/2)
  
  # Create results dataframe
  results <- data.frame(
    coefficient = coef_names,
    estimate = original_coefs,
    se = original_se,
    ci_lower = original_ci[, 1],
    ci_upper = original_ci[, 2],
    boot_mean = boot_means,
    boot_se = boot_se,
    boot_ci_lower = boot_ci_lower,
    boot_ci_upper = boot_ci_upper
  )
  
  return(results)
}

# Print the results
test_results <- bootstrap_lm(filtered_data, "logHomeRange ~ logBodyMass")
print(test_results)
```