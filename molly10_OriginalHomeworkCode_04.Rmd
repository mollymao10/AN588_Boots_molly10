---
title: "molly10_OriginalHomeworkCode_05"
author: "Yinuo Mao"
date: "4/10/2025"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(boot)
```

### Using the â€œKamilarAndCooperData.csvâ€ dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your ğ›½ coeffiecients (slope and intercept).
```{r}
data <- read.csv("KamilarAndCooperData.csv", header = TRUE, stringsAsFactors = FALSE)

# Basic examination
str(data)  # Structure of the data
head(data)  # First few rows

# Check relevant variables
summary(data$HomeRange_km2)
summary(data$Body_mass_female_mean)

# Filter and transform
filtered_data <- data %>%
  filter(!is.na(HomeRange_km2) & !is.na(Body_mass_female_mean))
filtered_data$logHomeRange <- log(filtered_data$HomeRange_km2)
filtered_data$logBodyMass <- log(filtered_data$Body_mass_female_mean)

# make the linear model
model <- lm(logHomeRange ~ logBodyMass, data = filtered_data)

# Display the summary of the model
summary(model)

# Extract the coefficients
coef(model)

# Get confidence intervals
confint(model)

# Plot the model
ggplot(filtered_data, aes(x = logBodyMass, y = logHomeRange)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    x = "Log(Body Mass of Female)",
    y = "Log(Home Range)",
    title = "Relationship between Log Body Mass and Log Home Range"
  ) +
  theme_minimal()
```

### Use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each ğ›½ coefficient.Estimate the standard error for each of your ğ›½ coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your ğ›½ coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
# Set seed for reproducibility
set.seed(123)

# Number of bootstrap replicates
n_boot <- 1000

# Initialize vectors to store the bootstrap estimates
boot_intercepts <- numeric(n_boot)
boot_slopes <- numeric(n_boot)

# Perform bootstrap
for (i in 1:n_boot) {
  # Sample with replacement
  boot_indices <- sample(nrow(filtered_data), replace = TRUE)
  boot_sample <- filtered_data[boot_indices, ]
  
  # Fit the model on bootstrap sample
  boot_model <- lm(logHomeRange ~ logBodyMass, data = boot_sample)
  
  # Store the coefficients
  boot_intercepts[i] <- coef(boot_model)[1]
  boot_slopes[i] <- coef(boot_model)[2]
}

# Calculate the mean of the bootstrap estimates
mean_boot_intercept <- mean(boot_intercepts)
mean_boot_slope <- mean(boot_slopes)

# Calculate bootstrap standard errors
se_boot_intercept <- sd(boot_intercepts)
se_boot_slope <- sd(boot_slopes)

# Calculate bootstrap confidence intervals (95%)
ci_boot_intercept <- quantile(boot_intercepts, c(0.025, 0.975))
ci_boot_slope <- quantile(boot_slopes, c(0.025, 0.975))

# Compare bootstrap results with original model
boot_results <- data.frame(
  Parameter = c("Intercept", "Slope"),
  Original_Estimate = coef(model),
  Original_SE = sqrt(diag(vcov(model))),
  Original_CI_Lower = confint(model)[,1],
  Original_CI_Upper = confint(model)[,2],
  Bootstrap_Mean = c(mean_boot_intercept, mean_boot_slope),
  Bootstrap_SE = c(se_boot_intercept, se_boot_slope),
  Bootstrap_CI_Lower = c(ci_boot_intercept[1], ci_boot_slope[1]),
  Bootstrap_CI_Upper = c(ci_boot_intercept[2], ci_boot_slope[2])
)

# Display the comparison
boot_results
```

### How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?
For the intercept coefficient, the bootstrap SE is smaller than the SE from the linear model. This suggests that the linear model might be slightly overestimating the uncertainty in the intercept. For the slope coefficient, the bootstrap SE is a little closer to the the SE from the linear model compared with the intercept coefficient but still about 5-10% smaller. The smaller bootstrap SEs suggest that the data may actually have less variability than what would be expected under the strict parametric assumptions, particularly for the intercept estimate.

### How does the latter compare to the 95% CI estimated from your entire dataset?
For the intercept coefficient, the bootstrap CI is slightly narrower and less symmetric than the parametric CI, with a higher lower bound, means there is more certainty about the minimum intercept value. Similarly, for the slope coefficient, the bootstrap CI has a higher lower bound while remains a similar upper bound compared to the parametric CI, indicating greater confidence in the minimum strength of the relationship between body mass and home range size. These patterns suggest that linear model, which rely on assumptions of normality and homoscedasticity, may be slightly overestimating the uncertainty in our regression parameters. However, the bootstrap method, by directly sampling from the observed data without making these assumptions, provide a more data-driven representation of uncertainty that show a stronger and more definite relationship between primate body mass and home range size than indicated by standard linear model CIs.

# EXTRA CREDIT
### Write a FUNCTION that takes as its arguments a dataframe, â€œdâ€, a linear model, â€œmâ€ (as a character string, e.g., â€œlogHR~logBMâ€), a user-defined confidence interval level, â€œconf.levelâ€ (with default = 0.95), and a number of bootstrap replicates, â€œnâ€ (with default = 1000). Your function should return a dataframe that includes: beta coefficient names; beta coefficients, standard errors, and upper and lower CI limits for the linear model based on your entire dataset; and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.
```{r}
bootstrap_lm <- function(d, m, conf.level = 0.95, n = 1000) {
  # Convert model formula from character to formula
  m_formula <- as.formula(m)
  
  # Fit the model on the full dataset
  full_model <- lm(m_formula, data = d)
  
  # Extract coefficients and their names
  coef_names <- names(coef(full_model))
  coef_values <- coef(full_model)
  
  # Get standard errors and confidence intervals for the full model
  coef_se <- sqrt(diag(vcov(full_model)))
  coef_ci <- confint(full_model, level = conf.level)
  
  # Initialize matrices to store bootstrap coefficients
  boot_coefs <- matrix(NA, nrow = n, ncol = length(coef_values))
  colnames(boot_coefs) <- coef_names
  
  # Perform bootstrap
  for (i in 1:n) {
    # Sample with replacement
    boot_indices <- sample(nrow(d), replace = TRUE)
    boot_sample <- d[boot_indices, ]
    
    # Fit the model on bootstrap sample
    boot_model <- lm(m_formula, data = boot_sample)
    
    # Store the coefficients
    boot_coefs[i, ] <- coef(boot_model)
  }
  
  # Calculate bootstrap means
  boot_means <- colMeans(boot_coefs)
  
  # Calculate bootstrap standard errors
  boot_se <- apply(boot_coefs, 2, sd)
  
  # Calculate bootstrap confidence intervals
  alpha <- 1 - conf.level
  boot_ci_lower <- apply(boot_coefs, 2, quantile, probs = alpha/2)
  boot_ci_upper <- apply(boot_coefs, 2, quantile, probs = 1 - alpha/2)
  
  # Create results dataframe
  results <- data.frame(
    coefficient = coef_names,
    estimate = coef_values,
    se = coef_se,
    ci_lower = coef_ci[, 1],
    ci_upper = coef_ci[, 2],
    boot_mean = boot_means,
    boot_se = boot_se,
    boot_ci_lower = boot_ci_lower,
    boot_ci_upper = boot_ci_upper
  )
  
  return(results)
}

# Test the function
bootstrap_results <- bootstrap_lm(filtered_data, "logHomeRange ~ logBodyMass")
bootstrap_results
```

### Graph each beta value from the linear model and its corresponding mean value, lower CI and upper CI from a bootstrap as a function of number of bootstraps from 10 to 200 by 10s. HINT: the beta value from the linear model will be the same for all bootstraps and the mean beta value may not differ that much!
```{r}
# Create a function to run bootstraps for different n values
bootstrap_by_n <- function(d, m, n_values) {
  results_list <- list()
  
  for (n in n_values) {
    cat("Running bootstrap with n =", n, "...\n")
    results_list[[as.character(n)]] <- bootstrap_lm(d, m, n = n)
  }
  
  return(results_list)
}

# Run bootstraps for n from 10 to 200 by 10s
n_values <- seq(10, 200, by = 10)
bootstrap_n_results <- bootstrap_by_n(filtered_data, "logHomeRange ~ logBodyMass", n_values)

# Extract intercept values for plotting
intercept_data <- data.frame(
  n = n_values,
  lm_estimate = rep(coef(model)[1], length(n_values)),
  boot_mean = sapply(bootstrap_n_results, function(x) x$boot_mean[1]),
  boot_ci_lower = sapply(bootstrap_n_results, function(x) x$boot_ci_lower[1]),
  boot_ci_upper = sapply(bootstrap_n_results, function(x) x$boot_ci_upper[1])
)

# Extract slope values for plotting
slope_data <- data.frame(
  n = n_values,
  lm_estimate = rep(coef(model)[2], length(n_values)),
  boot_mean = sapply(bootstrap_n_results, function(x) x$boot_mean[2]),
  boot_ci_lower = sapply(bootstrap_n_results, function(x) x$boot_ci_lower[2]),
  boot_ci_upper = sapply(bootstrap_n_results, function(x) x$boot_ci_upper[2])
)

# Plot intercept results
intercept_plot <- ggplot(intercept_data, aes(x = n)) +
  geom_line(aes(y = lm_estimate), color = "blue", size = 1) +
  geom_line(aes(y = boot_mean), color = "red", size = 1) +
  geom_ribbon(aes(ymin = boot_ci_lower, ymax = boot_ci_upper), alpha = 0.2) +
  labs(
    x = "Number of Bootstrap Replicates",
    y = "Intercept Value",
    title = "Intercept Estimates vs. Number of Bootstrap Replicates"
  ) +
  theme_minimal()

# Plot slope results
slope_plot <- ggplot(slope_data, aes(x = n)) +
  geom_line(aes(y = lm_estimate), color = "blue", size = 1) +
  geom_line(aes(y = boot_mean), color = "red", size = 1) +
  geom_ribbon(aes(ymin = boot_ci_lower, ymax = boot_ci_upper), alpha = 0.2) +
  labs(
    x = "Number of Bootstrap Replicates",
    y = "Slope Value",
    title = "Slope Estimates vs. Number of Bootstrap Replicates"
  ) +
  theme_minimal()

# Display plots
intercept_plot
slope_plot
```

## Challenges
1. At first, I found it really hard to grasp what bootstrapping actually does and why we need to sample with replacement. It took me a while to understand that we're trying to simulate having multiple datasets.
2. Trying to write the bootstrap code from scratch with for-loops was super frustrating! I kept getting errors about data types and subsetting. The loop would either crash or run forever without giving me results.
3. Once I got all the numbers, figuring out what the differences between bootstrap SE and regular SE actually meant for my analysis wasn't straightforward. I wasn't sure if smaller bootstrap SEs were good or bad.
4. Creating the function to extract coefficients for the boot() function was tricky. I had to make sure it returned exactly what I needed in the right format.
5. Getting the confidence intervals to display nicely in my final comparison table was surprisingly difficult. I had to figure out how to extract the values from boot.ci() output and convert them to a readable format.